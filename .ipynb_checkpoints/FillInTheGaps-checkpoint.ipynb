{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill In The Gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the missing word\n",
    "\n",
    "Our goal is take a sentence with a missing word -- \n",
    " for example \"Mary eats a \\_\", and replace the blank with a 'reasonable' replacement word.\n",
    "This task is something that people do everyday, especially in a noisy setting or when speaking to someone with a thick accent. \n",
    "However, this task goes beyond just syntactic validity, since few people would guess that the sentence was 'Mary eats a laptop'. We need to inject some notion of semantics. However, we cannot just naively apply methods from WordNet, because we are missing the word that fits in the blank. Most WordNet methods are ways of mapping from one word to other words that are related in a particular way, eg hypernymy or synonymy. \n",
    "\n",
    "So in order to move forward, we should first come up with a way of distinguishing valid sentences in a way that lets us generate a missing word. We decided to adopt a model of language learning that is very similar to the notion of near-miss learning. We take a set of training sentences, use software from lab 3 to convert them into event structures, and group event structures together in a way that lets us generalize semantically valid sentences from our training data. This means that we are assuming that the training data is semantically valid. \n",
    "\n",
    "Below, we will experiment with different grouping and generalization strategies in order to determine semantically valid replacements for a missing word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/scottviteri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/scottviteri/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import semantic as s\n",
    "import semantic_rule_set\n",
    "import rules\n",
    "\n",
    "import en\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('treebank')\n",
    "\n",
    "training_sentences_file = 'Input/training.txt'\n",
    "gap_sentences_file = 'Input/testing.txt'\n",
    "\n",
    "with open(training_sentences_file, 'r') as f:\n",
    "    training_sentences = [x.strip() for x in f]\n",
    "with open(gap_sentences_file, 'r') as f:\n",
    "    gap_sentences = [x.strip() for x in f]\n",
    "\n",
    "sem = semantic_rule_set.SemanticRuleSet()\n",
    "sem = rules.addLexicon(sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Event Structures\n",
    "First we generate event structures from sentences, which we store in a list of dictionaries for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary eats the _', 'Mary _  the tomato']\n",
      "{'action': 'eat', 'patient': 'potato', 'tense': 'present', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'present', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'present', 'agent': 'Mary'}\n"
     ]
    }
   ],
   "source": [
    "print gap_sentences\n",
    "events = map(lambda sent: s.sentenceToEventDict(sem, sent), training_sentences)\n",
    "for e in events:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest Strategy: No Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'potato', 'tense': 'present', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'present', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'present', 'agent': 'Mary'}\n"
     ]
    }
   ],
   "source": [
    "#Parse each sentence in training data\n",
    "def train(sem, sentences, groupingProcedure):\n",
    "    event_list = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "           new_event_dict = s.sentenceToEventDict(sem, sentence)\n",
    "           event_list = groupingProcedure(event_list, new_event_dict)\n",
    "        except Exception as e:\n",
    "            # The parser did not return any parse trees.\n",
    "            raise\n",
    "    return event_list\n",
    "\n",
    "def keepSeparate(event_list, new_event_dict):\n",
    "    return event_list + [new_event_dict]\n",
    "\n",
    "event_groupings = train(sem, training_sentences, keepSeparate)\n",
    "for g in event_groupings:\n",
    "    print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Difference Groupings\n",
    "This is a very conservative form of grouping. If two training sentences the same event structure but differ across one feature, we group together the values of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'potato', 'tense': 'present', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'present', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'present', 'agent': 'Mary'}\n",
      "{'action': set(['eat']), 'tense': set(['present']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}\n"
     ]
    }
   ],
   "source": [
    "def groupIfOneDiff(event_list, new_event): #if different structure, do not match\n",
    "    #maybe only do after reaching a certain size\n",
    "    new_event_list = copy.deepcopy(event_list)\n",
    "    merged = False\n",
    "    #try merging in\n",
    "    for i in range(len(event_list)): #try to match with event_list[i]\n",
    "        event = event_list[i]\n",
    "        if set(event.keys()) == set(new_event.keys()):\n",
    "            unequal_count = 0\n",
    "            for feat in event.keys():\n",
    "                if new_event[feat] not in event[feat]:\n",
    "                    unequal_feat = feat\n",
    "                    unequal_count += 1\n",
    "            if unequal_count == 0: merged = True\n",
    "            elif unequal_count == 1: #merge into previous\n",
    "                new_event_list[i][unequal_feat].add(new_event[unequal_feat])\n",
    "                merged = True\n",
    "    #make new spot\n",
    "    if not merged:\n",
    "        new_event_list.append({k:set([v]) for k,v in new_event.iteritems()})\n",
    "    return new_event_list\n",
    "\n",
    "event_groupings = train(sem, training_sentences, groupIfOneDiff)\n",
    "for e in events:\n",
    "    print e\n",
    "for g in event_groupings:\n",
    "    print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above only produces one output grouping (as opposed to two groupings, composed of sentences (1,2) and (2,3)).\n",
    "This is because we are applying the groupings iteratively. We loop through the events, and compare the current event with the groupings that we have collected up to that point. The comparison in this case is not checking for equality between values of a common feature, but rather it is checking for inclusion of the current event's feature values within groupings of that feature.\n",
    "```python\n",
    "for feat in event.keys():\n",
    "    if new_event[feat] not in event[feat]:\n",
    "        unequal_feat = feat\n",
    "```\n",
    "This part of the previous method demonstrates this inclusion checking.\n",
    "\n",
    "This implies that the same training sentences, in different orders, can lead to different event groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John eats the potato', 'John eats the tomato', 'Mary eats the tomato']\n",
      "['Mary eats the tomato', 'John eats the tomato', 'John eats the potato']\n",
      "[{'action': set(['eat']), 'tense': set(['present']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}]\n",
      "[{'action': set(['eat']), 'tense': set(['present']), 'patient': set(['tomato']), 'agent': set(['John', 'Mary'])}, {'action': set(['eat']), 'patient': set(['tomato', 'potato']), 'tense': set(['present']), 'agent': set(['John'])}]\n"
     ]
    }
   ],
   "source": [
    "def rotate(lst): \n",
    "    return [lst[-1]] + lst[:-1]\n",
    "print training_sentences\n",
    "print training_sentences[::-1]\n",
    "event_groupings_1 = train(sem, training_sentences, groupIfOneDiff)\n",
    "event_groupings_2 = train(sem, rotate(training_sentences), groupIfOneDiff)\n",
    "\n",
    "print event_groupings_1 #creates 1 group\n",
    "print event_groupings_2 #creates 2 groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grouping pattern is related to near-miss learning:\n",
    "\n",
    "The reason that event_groupings_2 creates 2 instead of 1 grouping is that the 1st and 2nd sentence differ from each other is 2 ways, instead of just 1.\n",
    "    \n",
    "But maybe this is a bit too conservative of an assumption. Alternately we could try grouping when seeing two differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'present', 'agent': 'Mary'}\n",
      "{'action': 'eat', 'patient': 'potato', 'tense': 'present', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'present', 'agent': 'John'}\n",
      "[{'action': set(['eat']), 'tense': set(['present']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}]\n"
     ]
    }
   ],
   "source": [
    "def groupIfOneOrTwoDiffs(event_list, new_event): #if different structure, do not match\n",
    "    #maybe only do after reaching a certain size\n",
    "    new_event_list = copy.deepcopy(event_list)\n",
    "    merged = False\n",
    "    #try merging in\n",
    "    for i in range(len(event_list)): #try to match with event_list[i]\n",
    "        event = event_list[i]\n",
    "        if set(event.keys()) == set(new_event.keys()):\n",
    "            unequal_count = 0\n",
    "            for feat in event.keys():\n",
    "                if new_event[feat] not in event[feat]:\n",
    "                    if unequal_count == 0:\n",
    "                        unequal_feat_1 = feat\n",
    "                    if unequal_count == 1:\n",
    "                        unequal_feat_2 = feat\n",
    "                    unequal_count += 1\n",
    "            if unequal_count == 0: merged = True\n",
    "            elif unequal_count == 1: #merge into previous\n",
    "                new_event_list[i][unequal_feat_1].add(new_event[unequal_feat_1])\n",
    "                merged = True\n",
    "            elif unequal_count == 2: #merge into previous\n",
    "                new_event_list[i][unequal_feat_1].add(new_event[unequal_feat_1])\n",
    "                new_event_list[i][unequal_feat_2].add(new_event[unequal_feat_2])\n",
    "                merged = True\n",
    "    #make new spot\n",
    "    if not merged:\n",
    "        new_event_list.append({k:set([v]) for k,v in new_event.iteritems()})\n",
    "    return new_event_list\n",
    "\n",
    "events = map(lambda sent: s.sentenceToEventDict(sem, sent), rotate(training_sentences))\n",
    "for e in events:\n",
    "    print e\n",
    "event_groupings = train(sem, rotate(training_sentences), groupIfOneOrTwoDiffs)\n",
    "print event_groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling In the Word Blank\n",
    "Now that we have come up with some event grouping strategies, we return to the original goal of our project -- to fill in the missing word.\n",
    "\n",
    "The reason for the groupings above, is that we would like to take training event structures like:\n",
    "```python\n",
    "['John eats the potato', 'John eats the tomato', 'Mary eats the tomato']\n",
    "```\n",
    "And conclude that:\n",
    "```python\n",
    "['Mary eats the potato']\n",
    "```\n",
    "is a valid sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: {'action': 'eat', 'patient': 'potato', 'tense': 'present', 'agent': 'Mary'}\n",
      "Event Groupings: [{'action': set(['eat']), 'tense': set(['present']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def checkGoodSentence(sem, sentence, event_groupings):\n",
    "    event = s.sentenceToEventDict(sem, sentence)\n",
    "    if not event: return False\n",
    "    for event_group in event_groupings:\n",
    "        if set(event.keys()) == set(event_group.keys()):\n",
    "            if all([event[k] in event_group[k] for k in event.keys()]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "sentence = 'Mary eats the potato'\n",
    "event_groupings = train(sem, training_sentences, groupIfOneDiff)\n",
    "event = s.sentenceToEventDict(sem, sentence)\n",
    "print \"Event: \" + str(event)\n",
    "print \"Event Groupings: \" + str(event_groupings)\n",
    "\n",
    "print checkGoodSentence(sem, sentence, event_groupings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the above means is that the grouping structure that we have generated 'accepts' the sentence 'Mary eats the potato' after being trained on the 3 sentences above, which is exactly what we were looking for!\n",
    "\n",
    "However, we want to be able to hypothesize that 'potato' is a good word to fill in for 'Mary eats the \\_'. So instead of starting with 'Mary eats the potato', let's start with 'Mary eats the \\_', and check the semantic validity of every word in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potato', 'tomato']\n"
     ]
    }
   ],
   "source": [
    "def gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings):\n",
    "    good_hypotheses = []\n",
    "    guess_words = s.getTerminals(sem) #all words in the lexicon\n",
    "    filler_i = gap_sentence.index('_')\n",
    "    \n",
    "    try:\n",
    "        for guess_word in guess_words:\n",
    "            guess_sentence = gap_sentence.replace('_',guess_word)\n",
    "            if checkGoodSentence(sem, guess_sentence, event_groupings):\n",
    "                good_hypotheses.append(guess_word)\n",
    "    except: \n",
    "        pass\n",
    "    return good_hypotheses\n",
    "\n",
    "filler_word_guesses = gapSentenceToFillerWordGuesses(sem, 'Mary eats the _', event_groupings)\n",
    "print filler_word_guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is doing what we want, but ideally we would like to extend our groupings to more than just the lexicon that we have written down.\n",
    "\n",
    "This is where we can use wordnet to generalize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Mary Eats: ['potato', 'tomato']\n",
      "What Mary Eats Extended: [u'potato', u'white_potato', u'Irish_potato', u'murphy', u'spud', u'tater', u'potato', u'white_potato', u'white_potato_vine', u'Solanum_tuberosum', u'tomato', u'tomato', u'love_apple', u'tomato_plant', u'Lycopersicon_esculentum']\n",
      "\n",
      "Mary eats the potato\n",
      "Mary eats the white_potato\n",
      "Mary eats the Irish_potato\n",
      "Mary eats the murphy\n",
      "Mary eats the spud\n",
      "Mary eats the tater\n",
      "Mary eats the potato\n",
      "Mary eats the white_potato\n",
      "Mary eats the white_potato_vine\n",
      "Mary eats the Solanum_tuberosum\n",
      "Mary eats the tomato\n",
      "Mary eats the tomato\n",
      "Mary eats the love_apple\n",
      "Mary eats the tomato_plant\n",
      "Mary eats the Lycopersicon_esculentum\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def flatten(lst):\n",
    "    out = []\n",
    "    for x in lst:\n",
    "        if type(x) is list:\n",
    "            out.extend(flatten(x))\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "gap_sentence = 'Mary eats the _'\n",
    "what_mary_eats = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "print \"What Mary Eats: \" + str(what_mary_eats)\n",
    "synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w)), what_mary_eats))\n",
    "print \"What Mary Eats Extended: \" + str(synonyms)\n",
    "print \n",
    "\n",
    "sents = ['Mary eats the '+syn for syn in synonyms]\n",
    "for x in sents: print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I try to replace the verb using synonyms?\n",
    "I lose tense information, and create ungrammatical sentences.\n",
    "\n",
    "Eg 'Mary _ the tomato' --> 'Mary chow the tomato'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary potato the tomato\n",
      "Mary white_potato the tomato\n",
      "Mary Irish_potato the tomato\n",
      "Mary murphy the tomato\n",
      "Mary spud the tomato\n",
      "Mary tater the tomato\n",
      "Mary potato the tomato\n",
      "Mary white_potato the tomato\n",
      "Mary white_potato_vine the tomato\n",
      "Mary Solanum_tuberosum the tomato\n",
      "Mary tomato the tomato\n",
      "Mary tomato the tomato\n",
      "Mary love_apple the tomato\n",
      "Mary tomato_plant the tomato\n",
      "Mary Lycopersicon_esculentum the tomato\n"
     ]
    }
   ],
   "source": [
    "gap_sentence = 'Mary _ the tomato'\n",
    "what_mary_did = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "verb_synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w, wn.VERB)), what_mary_did))\n",
    "\n",
    "for verb in synonyms:\n",
    "    sentence = 'Mary '+verb+' the tomato'\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential attempt at a fix would be to use an off-the-shelf parser to check the syntactic validity of each proposed sentence.\n",
    "\n",
    "Below, we try this with the Penn Tree Bank grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VBP', ['has']), Tree('NP-TMP-CLR', [Tree('NN', ['food'])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-TMP-CLR', [Tree('NN', ['food'])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBZ', ['has']), Tree('NP-PRD', [Tree('NN', ['food'])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBZ', ['has']), Tree('NP-PRD', [Tree('NP', [Tree('NN', ['food'])])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBZ', ['has']), Tree('NP-PRD', [Tree('NP', [Tree('NP', [Tree('NN', ['food'])])])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-PRD', [Tree('NN', ['food'])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-PRD', [Tree('NP', [Tree('NN', ['food'])])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-PRD', [Tree('NP', [Tree('NP', [Tree('NN', ['food'])])])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBZ', ['has']), Tree('NP-CLR', [Tree('NN', ['food'])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-CLR', [Tree('NN', ['food'])])])])])]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk.grammar import CFG, Nonterminal\n",
    "\n",
    "tbank_productions = set(production for sent in treebank.parsed_sents()\n",
    "                        for production in sent.productions())\n",
    "tbank_grammar = CFG(Nonterminal('S'), list(tbank_productions))\n",
    "parser = nltk.parse.EarleyChartParser(tbank_grammar)\n",
    "print list(parser.parse('Mary has food'.split()))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it looks like the above is effectively useless for our purposes.\n",
    "It will only have the words in the small subset of PTB that we can download, and takes very long to parse even 'Mary has food'. \n",
    "\n",
    "Let's try CMU's link grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -c -g -O -w -Iinclude src/parse.c -o obj/parse.o\n",
      "gcc -c -g -O -w -Iinclude src/prune.c -o obj/prune.o\n",
      "gcc -c -g -O -w -Iinclude src/and.c -o obj/and.o\n",
      "gcc -c -g -O -w -Iinclude src/post-process.c -o obj/post-process.o\n",
      "gcc -c -g -O -w -Iinclude src/pp_lexer.c -o obj/pp_lexer.o\n",
      "gcc -c -g -O -w -Iinclude src/resources.c -o obj/resources.o\n",
      "gcc -c -g -O -w -Iinclude src/analyze-linkage.c -o obj/analyze-linkage.o\n",
      "gcc -c -g -O -w -Iinclude src/string-set.c -o obj/string-set.o\n",
      "gcc -c -g -O -w -Iinclude src/pp_linkset.c -o obj/pp_linkset.o\n",
      "gcc -c -g -O -w -Iinclude src/pp_knowledge.c -o obj/pp_knowledge.o\n",
      "gcc -c -g -O -w -Iinclude src/error.c -o obj/error.o\n",
      "gcc -c -g -O -w -Iinclude src/word-file.c -o obj/word-file.o\n",
      "gcc -c -g -O -w -Iinclude src/utilities.c -o obj/utilities.o\n",
      "gcc -c -g -O -w -Iinclude src/tokenize.c -o obj/tokenize.o\n",
      "gcc -c -g -O -w -Iinclude src/command-line.c -o obj/command-line.o\n",
      "gcc -c -g -O -w -Iinclude src/read-dict.c -o obj/read-dict.o\n",
      "gcc -c -g -O -w -Iinclude src/print.c -o obj/print.o\n",
      "gcc -c -g -O -w -Iinclude src/preparation.c -o obj/preparation.o\n",
      "gcc -c -g -O -w -Iinclude src/api.c -o obj/api.o\n",
      "gcc -c -g -O -w -Iinclude src/massage.c -o obj/massage.o\n",
      "gcc -c -g -O -w -Iinclude src/linkset.c -o obj/linkset.o\n",
      "gcc -c -g -O -w -Iinclude src/idiom.c -o obj/idiom.o\n",
      "gcc -c -g -O -w -Iinclude src/fast-match.c -o obj/fast-match.o\n",
      "gcc -c -g -O -w -Iinclude src/extract-links.c -o obj/extract-links.o\n",
      "gcc -c -g -O -w -Iinclude src/count.c -o obj/count.o\n",
      "gcc -c -g -O -w -Iinclude src/build-disjuncts.c -o obj/build-disjuncts.o\n",
      "gcc -c -g -O -w -Iinclude src/constituents.c -o obj/constituents.o\n",
      "gcc -c -g -O -w -Iinclude src/print-util.c -o obj/print-util.o\n",
      "gcc -O -g  obj/parse.o obj/prune.o obj/and.o obj/post-process.o obj/pp_lexer.o obj/resources.o obj/analyze-linkage.o obj/string-set.o obj/pp_linkset.o obj/pp_knowledge.o obj/error.o obj/word-file.o obj/utilities.o obj/tokenize.o obj/command-line.o obj/read-dict.o obj/print.o obj/preparation.o obj/api.o obj/massage.o obj/linkset.o obj/idiom.o obj/fast-match.o obj/extract-links.o obj/count.o obj/build-disjuncts.o obj/constituents.o obj/print-util.o  -o ./parse \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ./link-4.1b-mod/ && make -B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentences: [u'Mary eat the tomatoes', u'Mary eat the tomatoes', u'Mary feed the tomatoes', u'Mary eat the tomatoes', u'Mary eat the tomatoes', u'Mary eat_on the tomatoes', u'Mary consume the tomatoes', u'Mary eat_up the tomatoes', u'Mary use_up the tomatoes', u'Mary eat the tomatoes', u'Mary deplete the tomatoes', u'Mary exhaust the tomatoes', u'Mary run_through the tomatoes', u'Mary wipe_out the tomatoes', u'Mary corrode the tomatoes', u'Mary eat the tomatoes', u'Mary rust the tomatoes', u'Mary eat the tomatoes', u'Mary eat the tomatoes', u'Mary feed the tomatoes', u'Mary eat the tomatoes', u'Mary eat the tomatoes', u'Mary eat_on the tomatoes', u'Mary consume the tomatoes', u'Mary eat_up the tomatoes', u'Mary use_up the tomatoes', u'Mary eat the tomatoes', u'Mary deplete the tomatoes', u'Mary exhaust the tomatoes', u'Mary run_through the tomatoes', u'Mary wipe_out the tomatoes', u'Mary corrode the tomatoes', u'Mary eat the tomatoes', u'Mary rust the tomatoes']\n",
      "\n",
      "Grammatical Sentences: []\n"
     ]
    }
   ],
   "source": [
    "def filterBySyntax(sentences):\n",
    "    with open(\"./link-4.1b-mod/input.txt\", \"w+\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence+'\\n')\n",
    "\n",
    "    wd = os.getcwd()\n",
    "    os.chdir(wd+\"/link-4.1b-mod\")\n",
    "    subprocess.call(['./parse'])\n",
    "    os.chdir(wd)\n",
    "\n",
    "    with open(\"./link-4.1b-mod/output.txt\", \"r\") as f:\n",
    "        syntactical_sentences = [x for x in f.read().split('\\n') if x != '']\n",
    "    return syntactical_sentences\n",
    "\n",
    "verb_synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w, wn.VERB)), what_mary_did))\n",
    "input_sentences = ['Mary '+verb+' the tomatoes' for verb in verb_synonyms]\n",
    "syntactical_sentences = filterBySyntax(input_sentences)\n",
    "print \"Input Sentences: \" + str(input_sentences)\n",
    "print\n",
    "\n",
    "print \"Grammatical Sentences: \" + str(syntactical_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did not let a single sentence through the grammar checker, as expected.\n",
    "\n",
    "Now let's use the NodeBox English Linguistics library to force conjugation of the proposed verbs, and then check our work with our grammar checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'eat', u'eat', u'feed', u'eat', u'eat', u'eat_on', u'consume', u'eat_up', u'use_up', u'eat']\n",
      "['eats', 'eats', 'fees', 'eats', 'eats', '', 'consumes', '', '', 'eats']\n"
     ]
    }
   ],
   "source": [
    "conjs = []\n",
    "for verb in verb_synonyms:\n",
    "    try:\n",
    "        conj = en.verb.present(verb, person=3, negate=False)\n",
    "        conjs.append(conj)\n",
    "    except Exception as e:\n",
    "        conjs.append('')\n",
    "\n",
    "print verb_synonyms[:10]\n",
    "print conjs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences with conjugated verb: \n",
      "['Mary eats the tomato', 'Mary eats the tomato', 'Mary fees the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary consumes the tomato', 'Mary eats the tomato', 'Mary depletes the tomato', 'Mary exhausts the tomato', 'Mary corrodes the tomato', 'Mary eats the tomato', 'Mary rusts the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary fees the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary consumes the tomato', 'Mary eats the tomato', 'Mary depletes the tomato', 'Mary exhausts the tomato', 'Mary corrodes the tomato', 'Mary eats the tomato', 'Mary rusts the tomato']\n",
      "\n",
      "after filter: \n",
      "['Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary consumes the tomato', 'Mary eats the tomato', 'Mary depletes the tomato', 'Mary exhausts the tomato', 'Mary corrodes the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary consumes the tomato', 'Mary eats the tomato', 'Mary depletes the tomato', 'Mary exhausts the tomato', 'Mary corrodes the tomato', 'Mary eats the tomato']\n",
      "\n",
      "removed: \n",
      "['Mary fees the tomato', 'Mary rusts the tomato', 'Mary fees the tomato', 'Mary rusts the tomato']\n"
     ]
    }
   ],
   "source": [
    "conj_sents = ['Mary '+conj+' the tomato' for conj in conjs if conj]\n",
    "\n",
    "print \"sentences with conjugated verb: \"\n",
    "print conj_sents\n",
    "print \"\"\n",
    "\n",
    "filtered = filterBySyntax(conj_sents)\n",
    "print \"after filter: \" \n",
    "print filtered\n",
    "\n",
    "print \"\"\n",
    "print \"removed: \"\n",
    "print [x for x in conj_sents if x not in filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finally allows us to come up with generalized alternatives for 'Mary _  the tomato'!\n",
    "The problem with the above method; however, is that we had to manually specify that the blank was supposed to be 3rd-person verb.\n",
    "\n",
    "We need to get away from this if we would like to generalize to English in general. Ideally, we could look at the event structures that we generate from our original grouping procedure, and use that information to automate the conjugation/modification of our generated sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'present', 'agent': 'Mary'}\n"
     ]
    }
   ],
   "source": [
    "#start_sentence = 'Mary _ the tomato'\n",
    "#filler_word_guesses = test(sem, [start_sentence], event_groupings)\n",
    "#print filler_word_guesses\n",
    "#what_mary_did = filler_word_guesses['Mary _ the tomato']\n",
    "what_mary_did_sentences = ['Mary '+verb+\" the tomato\" for verb in what_mary_did]\n",
    "generated_event_structures = [s.sentenceToEventDict(sem, sentence) for sentence in what_mary_did_sentences]\n",
    "print generated_event_structures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree(Start[], [Tree(S[], [Tree(NP[-pro, -wh], [Tree(Name[], ['Mary'])]), Tree(VP[], [Tree(V_args[], [Tree(V2[+tense], ['eats']), Tree(NP[-pro, -wh], [Tree(Det[], ['the']), Tree(APX[], []), Tree(N[-mass, number='singular'], ['tomato'])])])])])])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.parse_input_str(sem, what_mary_did_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Start[]\n",
      "  (S[]\n",
      "    (NP[-pro, -wh] (Name[] Mary))\n",
      "    (VP[]\n",
      "      (V_args[]\n",
      "        (V2[+tense] eats)\n",
      "        (NP[-pro, -wh]\n",
      "          (Det[] the)\n",
      "          (APX[] )\n",
      "          (N[-mass, number='singular'] tomato))))))\n"
     ]
    }
   ],
   "source": [
    "tree = s.parse_input_str(sem, what_mary_did_sentences[0])\n",
    "print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('eats', V2[+tense])\n",
      "\n",
      "[ *type* = 'V2' ]\n",
      "[ tense  = True ]\n"
     ]
    }
   ],
   "source": [
    "subtree = tree.pos()[1] # 2nd element of sentence was blank\n",
    "print subtree\n",
    "\n",
    "print \"\"\n",
    "features = subtree[1]\n",
    "print features\n",
    "#print tree.leaf_treeposition(1)\n",
    "#print tree[0][1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'potato', u'white_potato', u'Irish_potato', u'murphy', u'spud', u'tater', u'potato', u'white_potato', u'white_potato_vine', u'Solanum_tuberosum', u'tomato', u'tomato', u'love_apple', u'tomato_plant', u'Lycopersicon_esculentum'] \n",
      "\n",
      "[ *type* = 'V2' ]\n",
      "[ tense  = True ] \n",
      "\n",
      "[ *type* = 'V2' ]\n",
      "[ tense  = True ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'Mary _ the tomato'\n",
    "filler_index = test_sentence.split().index('_')\n",
    "print synonyms, '\\n'\n",
    "\n",
    "for sentence in what_mary_did_sentences:\n",
    "    tree = s.parse_input_str(sem, sentence)\n",
    "    features = tree.pos()[filler_index][1]\n",
    "    print features, '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the features, we can use these to inform the filtering of our synonyms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precompute\n",
    "gap_sentences = ['Mary _ the tomato', 'Mary eats the _']#, '_ eats the tomato']\n",
    "word_guesses = map(lambda gap_sentence: gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings), gap_sentences)\n",
    "sents_and_guesses = zip(gap_sentences,word_guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eats', 'eats', 'fees', 'eats', 'eats', 'consumes', 'eats', 'depletes', 'exhausts', 'corrodes', 'eats', 'rusts', 'eats', 'eats', 'fees', 'eats', 'eats', 'consumes', 'eats', 'depletes', 'exhausts', 'corrodes', 'eats', 'rusts']\n",
      "[u'potato', u'white_potato', u'Irish_potato', u'murphy', u'spud', u'tater', u'potato', u'white_potato', u'white_potato_vine', u'Solanum_tuberosum', u'tomato', u'tomato', u'love_apple', u'tomato_plant', u'Lycopersicon_esculentum']\n"
     ]
    }
   ],
   "source": [
    "def findPOS(features):\n",
    "    feat_keys = features.keys()\n",
    "    feat_type_index = filter(lambda (i,x): type(x) is nltk.featstruct.Feature, enumerate(feat_keys))[0][0]\n",
    "    feat_type = feat_keys[feat_type_index]\n",
    "    POS = features[feat_type]\n",
    "    return POS\n",
    "\n",
    "def getSynonymsSingleWord(word, POS):\n",
    "    if POS == 'N':\n",
    "        return flatten([synset.lemma_names() for synset in wn.synsets(word, wn.NOUN)])\n",
    "    elif POS[0] == 'V':\n",
    "        return flatten([synset.lemma_names() for synset in wn.synsets(word, wn.VERB)])\n",
    "    return flatten([synset.lemma_names() for synset in wn.synsets(word, wn.NOUN)]) #future work\n",
    "        \n",
    "    \n",
    "def getSynonyms(filler_words, POS):\n",
    "    return flatten(map(lambda word: getSynonymsSingleWord(word, POS), filler_words))\n",
    "\n",
    "def conjugate(word, POS_info):\n",
    "    features, subj_features = POS_info['features'], POS_info['subj_features']\n",
    "    POS, subj_POS = POS_info['POS'], POS_info['subj_POS']\n",
    "    conj = None\n",
    "    if POS[0] == 'V': #Verb\n",
    "        if features['tense']:\n",
    "            if subj_POS == 'N' and subj_features['number'] == 'plural':\n",
    "                conj = en.verb.conjugate(word, tense=\"present plural\")\n",
    "            elif subj_POS == 'N' or subj_POS == 'Name':\n",
    "                conj = en.verb.conjugate(word, tense=\"3rd singular present\")\n",
    "        else:\n",
    "            conj = en.verb.infinitive(word)\n",
    "    elif POS == 'N':\n",
    "        if features['number'] == 'singular':\n",
    "            conj = en.noun.singular(word)\n",
    "        elif features['number'] == 'plural':\n",
    "            conj = en.noun.plural(word)\n",
    "    elif POS == 'Name':\n",
    "        conj = en.noun.singular(word)\n",
    "    if not conj:\n",
    "        conj = word\n",
    "    return conj\n",
    "\n",
    "def fillerWordToPOSInfo(filler_word, gap_sentence):\n",
    "    filler_index = gap_sentence.split().index('_')\n",
    "    filled_sentence = gap_sentence.replace('_',filler_word)\n",
    "    tree = s.parse_input_str(sem, filled_sentence)\n",
    "    features, subj_features = tree.pos()[filler_index][1], tree.pos()[0][1]\n",
    "    POS, subj_POS = findPOS(features), findPOS(subj_features)\n",
    "    POS_info = {'features':features, 'subj_features':subj_features, \n",
    "                'POS': POS, 'subj_POS':subj_POS}\n",
    "    return POS_info\n",
    "        \n",
    "#conjugate needs all info, but generalize should only need one POS\n",
    "def generalizeAndConjugate(sem, gap_sentence, filler_words, generalizationProcedure, filtering=False): \n",
    "    POS_info = fillerWordToPOSInfo(filler_words[0], gap_sentence)\n",
    "    \n",
    "    generalizations = generalizationProcedure(filler_words, POS_info['POS'])\n",
    "    conjugated_gens = []\n",
    "    for g in generalizations:\n",
    "        try:\n",
    "            conjugated_gens.append(conjugate(g, POS_info))\n",
    "        except Exception as e: pass\n",
    "    if filtering:\n",
    "        return filterBySyntax(conjugated_gens)\n",
    "    else: \n",
    "        return conjugated_gens\n",
    "\n",
    "for (gap_sentence, filler_word_guesses) in sents_and_guesses:\n",
    "    gens = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getSynonyms, False)\n",
    "    print gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary eats the tomato\n",
      "Mary eats the tomato\n",
      "Mary fees the tomato\n",
      "Mary eats the tomato\n",
      "Mary eats the tomato\n",
      "Mary consumes the tomato\n",
      "Mary eats the tomato\n",
      "Mary depletes the tomato\n",
      "Mary exhausts the tomato\n",
      "Mary corrodes the tomato\n",
      "Mary eats the tomato\n",
      "Mary rusts the tomato\n",
      "Mary eats the tomato\n",
      "Mary eats the tomato\n",
      "Mary fees the tomato\n",
      "Mary eats the tomato\n",
      "Mary eats the tomato\n",
      "Mary consumes the tomato\n",
      "Mary eats the tomato\n",
      "Mary depletes the tomato\n",
      "Mary exhausts the tomato\n",
      "Mary corrodes the tomato\n",
      "Mary eats the tomato\n",
      "Mary rusts the tomato\n",
      "Mary eats the tomato\n",
      "Mary eats the tomato\n",
      "Mary eats the potato\n",
      "Mary eats the white_potato\n",
      "Mary eats the Irish_potato\n",
      "Mary eats the murphy\n",
      "Mary eats the spud\n",
      "Mary eats the tater\n",
      "Mary eats the potato\n",
      "Mary eats the white_potato\n",
      "Mary eats the white_potato_vine\n",
      "Mary eats the Solanum_tuberosum\n",
      "Mary eats the tomato\n",
      "Mary eats the tomato\n",
      "Mary eats the love_apple\n",
      "Mary eats the tomato_plant\n",
      "Mary eats the Lycopersicon_esculentum\n",
      "Mary eats the potato\n",
      "Mary eats the tomato\n",
      "Count:  43 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "filled_sentences = []\n",
    "for (gap_sentence, filler_word_guesses) in sents_and_guesses:\n",
    "    gens = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getSynonyms, False)\n",
    "    for g in gens:\n",
    "        filled_sentences.append(gap_sentence.replace('_',g))\n",
    "    for fill in filler_word_guesses:\n",
    "        filled_sentences.append(gap_sentence.replace('_',fill))\n",
    "\n",
    "for x in filled_sentences:\n",
    "    print x\n",
    "print \"Count: \", len(filled_sentences), '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way of filtering down our suggested generalizations, we are more free to experiment with different generalization techniques.\n",
    "\n",
    "For example, what if we notice that the is something in common to many of the suggestions, whether they be tomato or potato?  Fundamentally, \"Mary eats the \\_\" should semantically match any food item.\n",
    "\n",
    "Following this idea, we introduce generalization by shared hypernymy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chews', 'masticates', 'manducates', 'jaws', 'swallows', 'chews', 'masticates', 'manducates', 'jaws', 'swallows']\n",
      "[u'vascular_plant', u'tracheophyte']\n"
     ]
    }
   ],
   "source": [
    "def getSharedHypernyms(filler_words): #only use on Nouns\n",
    "    pairwise_synsets = []\n",
    "    for i in range(len(filler_words)):\n",
    "        for j in range(i+1,len(filler_words)):\n",
    "            a,b = filler_words[i], filler_words[j]\n",
    "            pairwise_synsets.append(findLowest(a, b)[0])\n",
    "    return reduce(lambda x,y: x.lowest_common_hypernyms(y)[0], pairwise_synsets).lemma_names()\n",
    "\n",
    "def getEntailments(filler_words): #only use on Verbs\n",
    "    synsets = flatten([wn.synsets(filler_word, wn.VERB) for filler_word in filler_words])\n",
    "    ents = flatten(map(lambda syn: syn.entailments(), synsets))\n",
    "    return flatten([x.lemma_names() for x in ents])\n",
    "    \n",
    "def getSharedHypernymsOrEntailments(filler_words, POS):\n",
    "    if POS == 'N':\n",
    "        return getSharedHypernyms(filler_words)\n",
    "    elif POS[0] == 'V':\n",
    "        return getEntailments(filler_words)\n",
    "    return getSharedHypernyms(filler_words) #future work\n",
    "\n",
    "def findLowest(w1, w2): #only use on Nouns\n",
    "    a, b = wn.synsets(w1, wn.NOUN), wn.synsets(w2, wn.NOUN)\n",
    "        \n",
    "    low_depth, low_synset = -float('inf'), None\n",
    "    curr_x, curr_y = None, None\n",
    "    for x in a:\n",
    "            for y in b:\n",
    "                    syns = x.lowest_common_hypernyms(y)\n",
    "                    if syns:\n",
    "                        depth = syns[0].min_depth()\n",
    "                        if (depth and depth >= low_depth):\n",
    "                                low_depth = depth\n",
    "                                low_synset = syns\n",
    "                                curr_x, curr_y = x, y\n",
    "    return low_synset\n",
    "\n",
    "\n",
    "for (gap_sentence, filler_word_guesses) in sents_and_guesses:\n",
    "    gens = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getSharedHypernymsOrEntailments)\n",
    "    print gens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These generalizations are good, but do not cover very much ground. We are trying to create a long list of viable alternatively for a blank spot, and this method took us from two hypotheses to two hypotheses. We can ameliorate this by taking the transitive closure of hyponymy under these objects. Such a closure would be a generalization of each of our previous generalizations in this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chews', 'masticates', 'manducates', 'jaws', 'swallows', 'chews', 'masticates', 'manducates', 'jaws', 'swallows']\n",
      "[u'aquatic_plant', u'water_plant', u'hydrophyte', u'hydrophytic_plant', u'bulbous_plant', u'cormous_plant', u'creeper', u'cultivar', u'cultivated_plant', u'deciduous_plant']\n"
     ]
    }
   ],
   "source": [
    "def getHyponymClosure(filler_words): #Noun\n",
    "    pairwise_synsets = []\n",
    "    for i in range(len(filler_words)):\n",
    "        for j in range(i+1,len(filler_words)):\n",
    "            a,b = filler_words[i], filler_words[j]\n",
    "            pairwise_synsets.append(findLowest(a, b)[0])\n",
    "    hypernym_synset = reduce(lambda x,y: x.lowest_common_hypernyms(y)[0], pairwise_synsets)\n",
    "    closure = flatten(hypernym_synset.closure(lambda y: y.hyponyms()))\n",
    "    return flatten(map(lambda x: x.lemma_names(), closure))\n",
    "\n",
    "def getHyponymsClosureOrEntailments(filler_words, POS):\n",
    "    if POS == 'N':\n",
    "        return getHyponymClosure(filler_words)\n",
    "    elif POS[0] == 'V':\n",
    "        return getEntailments(filler_words)\n",
    "    return getHyponymClosure(filler_words) #future work\n",
    "\n",
    "for (gap_sentence, filler_word_guesses) in sents_and_guesses:\n",
    "    gens = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getHyponymsClosureOrEntailments)\n",
    "    print gens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyponym closure of shared hyponyms works very well for nouns. However, when we apply the same technique on verbs, we find that many times there are no hypernyms. We may choose to be selective -- using hyponym closure for nouns and entailment for verbs.\n",
    "\n",
    "We can fill the currently unused word_type argument of getHyponymClosure to  implement differing generalization behavior based on the type of word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences_file = 'Input/training.txt'\n",
    "gap_sentences_file = 'Input/testing.txt'\n",
    "output_dir = 'Output/'\n",
    "\n",
    "with open(training_sentences_file, 'r') as f:\n",
    "    training_sentences = [x.strip() for x in f]\n",
    "with open(gap_sentences_file, 'r') as f:\n",
    "    gap_sentences = [x.strip() for x in f]\n",
    "\n",
    "\n",
    "sem = semantic_rule_set.SemanticRuleSet()\n",
    "sem = rules.addLexicon(sem)\n",
    "\n",
    "def fillInTheGaps(training_sentences, gap_sentences, groupingProcedure, generalizationProcedure):\n",
    "    event_groupings = train(sem, training_sentences, groupingProcedure)\n",
    "    with open(output_dir+groupingProcedure.__name__+generalizationProcedure.__name__+\".txt\", 'w+') as f:\n",
    "        for gap_sentence in gap_sentences:\n",
    "            filler_word_guesses = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "            gens = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, generalizationProcedure)\n",
    "            for g in gens:\n",
    "                f.write(gap_sentence.replace('_', g)+'\\n')\n",
    "\n",
    "fillInTheGaps(training_sentences, gap_sentences, groupIfOneDiff, getSynonyms)\n",
    "fillInTheGaps(training_sentences, gap_sentences, groupIfOneOrTwoDiffs, getSynonyms)\n",
    "fillInTheGaps(training_sentences, gap_sentences, groupIfOneDiff, getHyponymsClosureOrEntailments)\n",
    "fillInTheGaps(training_sentences, gap_sentences, groupIfOneOrTwoDiffs, getHyponymsClosureOrEntailments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary eats the potato\n",
      "Mary eats the white_potato\n",
      "Mary eats the Irish_potato\n",
      "Mary eats the murphy\n",
      "Mary eats the spud\n",
      "Mary eats the tater\n",
      "Mary eats the potato\n",
      "Mary eats the white_potato\n",
      "Mary eats the white_potato_vine\n",
      "Mary eats the Solanum_tuberosum\n",
      "Mary fees  the tomato\n",
      "Mary eats  the tomato\n",
      "Mary eats  the tomato\n",
      "Mary consumes  the tomato\n",
      "Mary eats  the tomato\n",
      "Mary depletes  the tomato\n",
      "Mary exhausts  the tomato\n",
      "Mary corrodes  the tomato\n",
      "Mary eats  the tomato\n",
      "Mary rusts  the tomato\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "head -10 ./Output/groupIfOneDiffgetSynonyms.txt\n",
    "tail -10 ./Output/groupIfOneDiffgetSynonyms.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary eats the aquatic_plant\n",
      "Mary eats the water_plant\n",
      "Mary eats the hydrophyte\n",
      "Mary eats the hydrophytic_plant\n",
      "Mary eats the bulbous_plant\n",
      "Mary eats the cormous_plant\n",
      "Mary eats the creeper\n",
      "Mary eats the cultivar\n",
      "Mary eats the cultivated_plant\n",
      "Mary eats the deciduous_plant\n",
      "Mary chews  the tomato\n",
      "Mary masticates  the tomato\n",
      "Mary manducates  the tomato\n",
      "Mary jaws  the tomato\n",
      "Mary swallows  the tomato\n",
      "Mary chews  the tomato\n",
      "Mary masticates  the tomato\n",
      "Mary manducates  the tomato\n",
      "Mary jaws  the tomato\n",
      "Mary swallows  the tomato\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "head -10 ./Output/groupIfOneDiffgetHyponymsClosureOrEntailments.txt\n",
    "tail -10 ./Output/groupIfOneDiffgetHyponymsClosureOrEntailments.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading!\n",
    "\n",
    "Future Work:\n",
    "\n",
    "1. Grouping procedures that use synsets during the iterative grouping. \n",
    "\n",
    "2. Grouping procedures that only group according to n/log(n) human learning rule\n",
    "\n",
    "3. Extending supported parts of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions/Limitations:\n",
    " 1. Can currently only replace nouns or verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
