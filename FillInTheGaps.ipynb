{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import semantic as s\n",
    "import semantic_rule_set\n",
    "import rules\n",
    "\n",
    "import en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "training_sentences_file = 'training.txt'\n",
    "gap_sentences_file = 'testing.txt'\n",
    "\n",
    "with open(training_sentences_file, 'r') as f:\n",
    "    training_sentences = [x.strip() for x in f]\n",
    "with open(gap_sentences_file, 'r') as f:\n",
    "    gap_sentences = [x.strip() for x in f]\n",
    "\n",
    "sem = semantic_rule_set.SemanticRuleSet()\n",
    "sem = rules.addLexicon(sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the missing word\n",
    "\n",
    "Our goal is take a sentence with a missing word -- \n",
    " for example \"Mary ate a \\_\", and replace the blank with a 'reasonable' replacement word.\n",
    "This task is something that people do everyday, especially in a noisy setting or when speaking to someone with a thick accent. \n",
    "However, this task goes beyond just syntactic validity, since few people would guess that the sentence was 'Mary ate a laptop'. We need to inject some notion of semantics. However, we cannot just naively apply methods from WordNet, because we are missing the word that fits in the blank. Most WordNet methods are ways of mapping from one word to other words that are related in a particular way, eg hypernymy or synonymy. \n",
    "\n",
    "So in order to move forward, we should first come up with a way of distinguishing valid sentences in a way that lets us generate a missing word. We decided to adopt a model of language learning that is very similar to the notion of near-miss learning. We take a set of training sentences, use software from lab 3 to convert them into event structures, and group event structures together in a way that lets us generalize semantically valid sentences from our training data. This means that we are assuming that the training data is semantically valid. \n",
    "\n",
    "Below, we will experiment with different grouping and generalization strategies in order to determine semantically valid replacements for a missing word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Event Structures\n",
    "First we generate event structures from sentences, which we store in a list of dictionaries for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary _ the potato', 'Mary ate the _']\n",
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'Mary'}\n"
     ]
    }
   ],
   "source": [
    "print gap_sentences\n",
    "events = map(lambda sent: s.sentenceToEventDict(sem, sent), training_sentences)\n",
    "for e in events:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest Strategy: No Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'Mary'}\n"
     ]
    }
   ],
   "source": [
    "#Parse each sentence in training data\n",
    "def train(sem, sentences, groupingProcedure):\n",
    "    event_list = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "           new_event_dict = s.sentenceToEventDict(sem, sentence)\n",
    "           event_list = groupingProcedure(event_list, new_event_dict)\n",
    "        except Exception as e:\n",
    "            # The parser did not return any parse trees.\n",
    "            raise\n",
    "    return event_list\n",
    "\n",
    "def keepSeparate(event_list, new_event_dict):\n",
    "    return event_list + [new_event_dict]\n",
    "\n",
    "event_groupings = train(sem, training_sentences, keepSeparate)\n",
    "for g in event_groupings:\n",
    "    print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Difference Groupings\n",
    "This is a very conservative form of grouping. If two training sentences the same event structure but differ across one feature, we group together the values of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'Mary'}\n",
      "{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}\n"
     ]
    }
   ],
   "source": [
    "def groupIfOneDiff(event_list, new_event): #if different structure, do not match\n",
    "    #maybe only do after reaching a certain size\n",
    "    new_event_list = copy.deepcopy(event_list)\n",
    "    merged = False\n",
    "    #try merging in\n",
    "    for i in range(len(event_list)): #try to match with event_list[i]\n",
    "        event = event_list[i]\n",
    "        if set(event.keys()) == set(new_event.keys()):\n",
    "            unequal_count = 0\n",
    "            for feat in event.keys():\n",
    "                if new_event[feat] not in event[feat]:\n",
    "                    unequal_feat = feat\n",
    "                    unequal_count += 1\n",
    "            if unequal_count == 0: merged = True\n",
    "            elif unequal_count == 1: #merge into previous\n",
    "                new_event_list[i][unequal_feat].add(new_event[unequal_feat])\n",
    "                merged = True\n",
    "    #make new spot\n",
    "    if not merged:\n",
    "        new_event_list.append({k:set([v]) for k,v in new_event.iteritems()})\n",
    "    return new_event_list\n",
    "\n",
    "event_groupings = train(sem, training_sentences, groupIfOneDiff)\n",
    "for e in events:\n",
    "    print e\n",
    "for g in event_groupings:\n",
    "    print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above only produces one output grouping (as opposed to two groupings, composed of sentences (1,2) and (2,3)).\n",
    "This is because we are applying the groupings iteratively. We loop through the events, and compare the current event with the groupings that we have collected up to that point. The comparison in this case is not checking for equality between values of a common feature, but rather it is checking for inclusion of the current event's feature values within groupings of that feature.\n",
    "```python\n",
    "for feat in event.keys():\n",
    "    if new_event[feat] not in event[feat]:\n",
    "        unequal_feat = feat\n",
    "```\n",
    "This part of the previous method demonstrates this inclusion checking.\n",
    "\n",
    "This implies that the same training sentences, in different orders, can lead to different event groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John ate the potato', 'John ate the tomato', 'Mary ate the tomato']\n",
      "['Mary ate the tomato', 'John ate the tomato', 'John ate the potato']\n",
      "[{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}]\n",
      "[{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato']), 'agent': set(['John', 'Mary'])}, {'action': set(['eat']), 'patient': set(['tomato', 'potato']), 'tense': set(['past']), 'agent': set(['John'])}]\n"
     ]
    }
   ],
   "source": [
    "def rotate(lst): \n",
    "    return [lst[-1]] + lst[:-1]\n",
    "print training_sentences\n",
    "print training_sentences[::-1]\n",
    "event_groupings_1 = train(sem, training_sentences, groupIfOneDiff)\n",
    "event_groupings_2 = train(sem, rotate(training_sentences), groupIfOneDiff)\n",
    "\n",
    "print event_groupings_1 #creates 1 group\n",
    "print event_groupings_2 #creates 2 groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grouping pattern is related to near-miss learning:\n",
    "\n",
    "The reason that event_groupings_2 creates 2 instead of 1 grouping is that the 1st and 2nd sentence differ from each other is 2 ways, instead of just 1.\n",
    "    \n",
    "But maybe this is a bit too conservative of an assumption. Alternately we could try grouping when seeing two differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'Mary'}\n",
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'John'}\n",
      "[{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}]\n"
     ]
    }
   ],
   "source": [
    "def groupIfOneOrTwoDiffs(event_list, new_event): #if different structure, do not match\n",
    "    #maybe only do after reaching a certain size\n",
    "    new_event_list = copy.deepcopy(event_list)\n",
    "    merged = False\n",
    "    #try merging in\n",
    "    for i in range(len(event_list)): #try to match with event_list[i]\n",
    "        event = event_list[i]\n",
    "        if set(event.keys()) == set(new_event.keys()):\n",
    "            unequal_count = 0\n",
    "            for feat in event.keys():\n",
    "                if new_event[feat] not in event[feat]:\n",
    "                    if unequal_count == 0:\n",
    "                        unequal_feat_1 = feat\n",
    "                    if unequal_count == 1:\n",
    "                        unequal_feat_2 = feat\n",
    "                    unequal_count += 1\n",
    "            if unequal_count == 0: merged = True\n",
    "            elif unequal_count == 1: #merge into previous\n",
    "                new_event_list[i][unequal_feat_1].add(new_event[unequal_feat_1])\n",
    "                merged = True\n",
    "            elif unequal_count == 2: #merge into previous\n",
    "                new_event_list[i][unequal_feat_1].add(new_event[unequal_feat_1])\n",
    "                new_event_list[i][unequal_feat_2].add(new_event[unequal_feat_2])\n",
    "                merged = True\n",
    "    #make new spot\n",
    "    if not merged:\n",
    "        new_event_list.append({k:set([v]) for k,v in new_event.iteritems()})\n",
    "    return new_event_list\n",
    "\n",
    "events = map(lambda sent: s.sentenceToEventDict(sem, sent), rotate(training_sentences))\n",
    "for e in events:\n",
    "    print e\n",
    "event_groupings = train(sem, rotate(training_sentences), groupIfOneOrTwoDiffs)\n",
    "print event_groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling In the Word Blank\n",
    "Now that we have come up with some event grouping strategies, we return to the original goal of our project -- to fill in the missing word.\n",
    "\n",
    "The reason for the groupings above, is that we would like to take training event structures like:\n",
    "```python\n",
    "['John ate the potato', 'John ate the tomato', 'Mary ate the tomato']\n",
    "```\n",
    "And conclude that:\n",
    "```python\n",
    "['Mary ate the potato']\n",
    "```\n",
    "is a valid sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'Mary'}\n",
      "[{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def checkGoodSentence(sem, sentence, event_groupings):\n",
    "    event = s.sentenceToEventDict(sem, sentence)\n",
    "    if not event: return False\n",
    "    for event_group in event_groupings:\n",
    "        if set(event.keys()) == set(event_group.keys()):\n",
    "            if all([event[k] in event_group[k] for k in event.keys()]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "sentence = 'Mary ate the potato'\n",
    "event_groupings = train(sem, training_sentences, groupIfOneDiff)\n",
    "event = s.sentenceToEventDict(sem, sentence)\n",
    "print event\n",
    "print event_groupings\n",
    "\n",
    "print checkGoodSentence(sem, sentence, event_groupings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the above means is that the grouping structure that we have generated 'accepts' the sentence 'Mary ate the potato' after being trained on the 3 sentences above, which is exactly what we were looking for!\n",
    "\n",
    "However, we want to be able to hypothesize that 'potato' is a good word to fill in for 'Mary ate the \\_'. So instead of starting with 'Mary ate the potato', let's start with 'Mary ate the \\_', and check the semantic validity of every word in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potato', 'tomato']\n"
     ]
    }
   ],
   "source": [
    "def gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings):\n",
    "    good_hypotheses = []\n",
    "    guess_words = s.getTerminals(sem) #all words in the lexicon\n",
    "    filler_i = gap_sentence.index('_')\n",
    "        \n",
    "    try:\n",
    "        for guess_word in guess_words:\n",
    "            guess_sentence = gap_sentence.replace('_',guess_word)\n",
    "            if checkGoodSentence(sem, guess_sentence, event_groupings):\n",
    "                good_hypotheses.append(guess_word)\n",
    "    except: pass\n",
    "    return good_hypotheses\n",
    "\n",
    "filler_word_guesses = gapSentenceToFillerWordGuesses(sem, 'Mary ate the _', event_groupings)\n",
    "print filler_word_guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is doing what we want, but ideally we would like to extend our groupings to more than just the lexicon that we have written down.\n",
    "\n",
    "This is where we can use wordnet to generalize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary ate the potato\n",
      "Mary ate the white_potato\n",
      "Mary ate the Irish_potato\n",
      "Mary ate the murphy\n",
      "Mary ate the spud\n",
      "Mary ate the tater\n",
      "Mary ate the potato\n",
      "Mary ate the white_potato\n",
      "Mary ate the white_potato_vine\n",
      "Mary ate the Solanum_tuberosum\n",
      "Mary ate the tomato\n",
      "Mary ate the tomato\n",
      "Mary ate the love_apple\n",
      "Mary ate the tomato_plant\n",
      "Mary ate the Lycopersicon_esculentum\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def flatten(lst):\n",
    "    out = []\n",
    "    for x in lst:\n",
    "        if type(x) is list:\n",
    "            out.extend(flatten(x))\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "gap_sentence = 'Mary ate the _'\n",
    "what_mary_ate = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w)), what_mary_ate))\n",
    "\n",
    "sents = ['Mary ate the '+syn for syn in synonyms]\n",
    "for x in sents: print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately see a problem with this method. In trying to generalize our semantic results, we lose even basic syntactic correctness.\n",
    "If I change the training sentences to present tense, I end up with noun synonyms.\n",
    "\n",
    "Note that here I am filtering by noun synonym class for clarity.\n",
    "\n",
    "Eg 'Mary _ the tomato' --> 'Mary chow the tomato'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun synonym: Mary Ate the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary feed the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary eat_on the tomato\n",
      "Verb synonym: Mary consume the tomato\n",
      "Verb synonym: Mary eat_up the tomato\n",
      "Verb synonym: Mary use_up the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary deplete the tomato\n",
      "Verb synonym: Mary exhaust the tomato\n",
      "Verb synonym: Mary run_through the tomato\n",
      "Verb synonym: Mary wipe_out the tomato\n",
      "Verb synonym: Mary corrode the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary rust the tomato\n",
      "Noun synonym: Mary Ate the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary feed the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary eat_on the tomato\n",
      "Verb synonym: Mary consume the tomato\n",
      "Verb synonym: Mary eat_up the tomato\n",
      "Verb synonym: Mary use_up the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary deplete the tomato\n",
      "Verb synonym: Mary exhaust the tomato\n",
      "Verb synonym: Mary run_through the tomato\n",
      "Verb synonym: Mary wipe_out the tomato\n",
      "Verb synonym: Mary corrode the tomato\n",
      "Verb synonym: Mary eat the tomato\n",
      "Verb synonym: Mary rust the tomato\n"
     ]
    }
   ],
   "source": [
    "gap_sentence = 'Mary _ the tomato'\n",
    "what_mary_did = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w)), what_mary_did))\n",
    "noun_synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w, wn.NOUN)), what_mary_did))\n",
    "\n",
    "for syn in synonyms:\n",
    "    sentence = 'Mary '+syn+' the tomato'\n",
    "    if syn in noun_synonyms:\n",
    "        print \"Noun synonym: \"+sentence\n",
    "    else:\n",
    "        print \"Verb synonym: \"+sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in this case, I can just as easily filter by verb, fixing the part of speech, but not the tense or the plurality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary eat the tomato\n",
      "Mary eat the tomato\n",
      "Mary feed the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat_on the tomato\n",
      "Mary consume the tomato\n",
      "Mary eat_up the tomato\n",
      "Mary use_up the tomato\n",
      "Mary eat the tomato\n",
      "Mary deplete the tomato\n",
      "Mary exhaust the tomato\n",
      "Mary run_through the tomato\n",
      "Mary wipe_out the tomato\n",
      "Mary corrode the tomato\n",
      "Mary eat the tomato\n",
      "Mary rust the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat the tomato\n",
      "Mary feed the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat_on the tomato\n",
      "Mary consume the tomato\n",
      "Mary eat_up the tomato\n",
      "Mary use_up the tomato\n",
      "Mary eat the tomato\n",
      "Mary deplete the tomato\n",
      "Mary exhaust the tomato\n",
      "Mary run_through the tomato\n",
      "Mary wipe_out the tomato\n",
      "Mary corrode the tomato\n",
      "Mary eat the tomato\n",
      "Mary rust the tomato\n"
     ]
    }
   ],
   "source": [
    "synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w, wn.VERB)), what_mary_did))\n",
    "\n",
    "sents = ['Mary '+syn+' the tomato' for syn in synonyms]\n",
    "for x in sents: print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential attempt at a fix would be to use an off-the-shelf parser to check the syntactic validity of each proposed sentence.\n",
    "\n",
    "Below, we try this with the Penn Tree Bank grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VBP', ['has']), Tree('NP-TMP-CLR', [Tree('NN', ['food'])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-TMP-CLR', [Tree('NN', ['food'])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBZ', ['has']), Tree('NP-PRD', [Tree('NN', ['food'])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBZ', ['has']), Tree('NP-PRD', [Tree('NP', [Tree('NN', ['food'])])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBZ', ['has']), Tree('NP-PRD', [Tree('NP', [Tree('NP', [Tree('NN', ['food'])])])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-PRD', [Tree('NN', ['food'])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-PRD', [Tree('NP', [Tree('NN', ['food'])])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-PRD', [Tree('NP', [Tree('NP', [Tree('NN', ['food'])])])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBZ', ['has']), Tree('NP-CLR', [Tree('NN', ['food'])])])])]), Tree('S', [Tree('NP-SBJ-8', [Tree('NNP', ['Mary'])]), Tree('VP', [Tree('VP', [Tree('VBP', ['has']), Tree('NP-CLR', [Tree('NN', ['food'])])])])])]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "from nltk.grammar import CFG, Nonterminal\n",
    "\n",
    "tbank_productions = set(production for sent in treebank.parsed_sents()\n",
    "                        for production in sent.productions())\n",
    "tbank_grammar = CFG(Nonterminal('S'), list(tbank_productions))\n",
    "parser = nltk.parse.EarleyChartParser(tbank_grammar)\n",
    "print list(parser.parse('Mary has food'.split()))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it looks like the above is effectively useless for our purposes.\n",
    "It will only have the words in the small subset of PTB that we can download, and takes very long to parse even 'Mary has food'. \n",
    "\n",
    "Let's try CMU's link grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -c -g -O -w -Iinclude src/parse.c -o obj/parse.o\n",
      "gcc -c -g -O -w -Iinclude src/prune.c -o obj/prune.o\n",
      "gcc -c -g -O -w -Iinclude src/and.c -o obj/and.o\n",
      "gcc -c -g -O -w -Iinclude src/post-process.c -o obj/post-process.o\n",
      "gcc -c -g -O -w -Iinclude src/pp_lexer.c -o obj/pp_lexer.o\n",
      "gcc -c -g -O -w -Iinclude src/resources.c -o obj/resources.o\n",
      "gcc -c -g -O -w -Iinclude src/analyze-linkage.c -o obj/analyze-linkage.o\n",
      "gcc -c -g -O -w -Iinclude src/string-set.c -o obj/string-set.o\n",
      "gcc -c -g -O -w -Iinclude src/pp_linkset.c -o obj/pp_linkset.o\n",
      "gcc -c -g -O -w -Iinclude src/pp_knowledge.c -o obj/pp_knowledge.o\n",
      "gcc -c -g -O -w -Iinclude src/error.c -o obj/error.o\n",
      "gcc -c -g -O -w -Iinclude src/word-file.c -o obj/word-file.o\n",
      "gcc -c -g -O -w -Iinclude src/utilities.c -o obj/utilities.o\n",
      "gcc -c -g -O -w -Iinclude src/tokenize.c -o obj/tokenize.o\n",
      "gcc -c -g -O -w -Iinclude src/command-line.c -o obj/command-line.o\n",
      "gcc -c -g -O -w -Iinclude src/read-dict.c -o obj/read-dict.o\n",
      "gcc -c -g -O -w -Iinclude src/print.c -o obj/print.o\n",
      "gcc -c -g -O -w -Iinclude src/preparation.c -o obj/preparation.o\n",
      "gcc -c -g -O -w -Iinclude src/api.c -o obj/api.o\n",
      "gcc -c -g -O -w -Iinclude src/massage.c -o obj/massage.o\n",
      "gcc -c -g -O -w -Iinclude src/linkset.c -o obj/linkset.o\n",
      "gcc -c -g -O -w -Iinclude src/idiom.c -o obj/idiom.o\n",
      "gcc -c -g -O -w -Iinclude src/fast-match.c -o obj/fast-match.o\n",
      "gcc -c -g -O -w -Iinclude src/extract-links.c -o obj/extract-links.o\n",
      "gcc -c -g -O -w -Iinclude src/count.c -o obj/count.o\n",
      "gcc -c -g -O -w -Iinclude src/build-disjuncts.c -o obj/build-disjuncts.o\n",
      "gcc -c -g -O -w -Iinclude src/constituents.c -o obj/constituents.o\n",
      "gcc -c -g -O -w -Iinclude src/print-util.c -o obj/print-util.o\n",
      "gcc -O -g  obj/parse.o obj/prune.o obj/and.o obj/post-process.o obj/pp_lexer.o obj/resources.o obj/analyze-linkage.o obj/string-set.o obj/pp_linkset.o obj/pp_knowledge.o obj/error.o obj/word-file.o obj/utilities.o obj/tokenize.o obj/command-line.o obj/read-dict.o obj/print.o obj/preparation.o obj/api.o obj/massage.o obj/linkset.o obj/idiom.o obj/fast-match.o obj/extract-links.o obj/count.o obj/build-disjuncts.o obj/constituents.o obj/print-util.o  -o ./parse \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ./link-4.1b-mod/ && make -B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def filterBySyntax(sentences):\n",
    "    with open(\"./link-4.1b-mod/input.txt\", \"w+\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence+'\\n')\n",
    "\n",
    "    wd = os.getcwd()\n",
    "    os.chdir(wd+\"/link-4.1b-mod\")\n",
    "    subprocess.call(['./parse'])\n",
    "    os.chdir(wd)\n",
    "\n",
    "    with open(\"./link-4.1b-mod/output.txt\", \"r\") as f:\n",
    "        syntactical_sentences = [x for x in f.read().split('\\n') if x != '']\n",
    "    return syntactical_sentences\n",
    "\n",
    "synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w)), what_mary_did))\n",
    "sentences = ['Mary '+syn+' the tomato' for syn in synonyms]\n",
    "syntactical_sentences = filterBySyntax(sentences)\n",
    "print syntactical_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be working better as a syntax filter, but we are again at a cross-roads. No sentence from our generalizations was grammatical!\n",
    "\n",
    "So we will have to be a bit more careful in how we handle this.\n",
    "\n",
    "For this particular case, we can use the NodeBox English Linguistics library to force conjugation of the proposed verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Ate', u'eat', u'eat', u'feed', u'eat']\n",
      "['', 'eats', 'eats', 'fees', 'eats']\n"
     ]
    }
   ],
   "source": [
    "conjs = []\n",
    "for syn in synonyms:\n",
    "    try:\n",
    "        conj = en.verb.present(syn, person=3, negate=False)\n",
    "        conjs.append(conj)\n",
    "    except Exception as e:\n",
    "        conjs.append('')\n",
    "\n",
    "print synonyms[0:5]\n",
    "print conjs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences with conjugated verb: \n",
      "['Mary eats the tomato', 'Mary eats the tomato', 'Mary fees the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary consumes the tomato', 'Mary eats the tomato', 'Mary depletes the tomato', 'Mary exhausts the tomato', 'Mary corrodes the tomato', 'Mary eats the tomato', 'Mary rusts the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary fees the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary consumes the tomato', 'Mary eats the tomato', 'Mary depletes the tomato', 'Mary exhausts the tomato', 'Mary corrodes the tomato', 'Mary eats the tomato', 'Mary rusts the tomato']\n",
      "\n",
      "after filter: \n",
      "['Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary consumes the tomato', 'Mary eats the tomato', 'Mary depletes the tomato', 'Mary exhausts the tomato', 'Mary corrodes the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary eats the tomato', 'Mary consumes the tomato', 'Mary eats the tomato', 'Mary depletes the tomato', 'Mary exhausts the tomato', 'Mary corrodes the tomato', 'Mary eats the tomato']\n",
      "\n",
      "removed: \n",
      "['Mary fees the tomato', 'Mary rusts the tomato', 'Mary fees the tomato', 'Mary rusts the tomato']\n"
     ]
    }
   ],
   "source": [
    "conj_sents = ['Mary '+conj+' the tomato' for conj in conjs if conj]\n",
    "\n",
    "print \"sentences with conjugated verb: \"\n",
    "print conj_sents\n",
    "print \"\"\n",
    "\n",
    "filtered = filterBySyntax(conj_sents)\n",
    "print \"after filter: \" \n",
    "print filtered\n",
    "\n",
    "print \"\"\n",
    "print \"removed: \"\n",
    "print [x for x in conj_sents if x not in filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finally allows us to come up with generalized alternatives for 'Mary _  the tomato'!\n",
    "The problem with the above method; however, is that we had to manually specify that the blank was supposed to be 3rd-person verb.\n",
    "\n",
    "We need to get away from this if we would like to generalize to English in general. Ideally, we could look at the event structures that we generate from our original grouping procedure, and use that information to automate the conjugation/modification of our generated sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'Mary'}\n"
     ]
    }
   ],
   "source": [
    "#start_sentence = 'Mary _ the tomato'\n",
    "#filler_word_guesses = test(sem, [start_sentence], event_groupings)\n",
    "#print filler_word_guesses\n",
    "#what_mary_did = filler_word_guesses['Mary _ the tomato']\n",
    "what_mary_did_sentences = ['Mary '+verb+\" the tomato\" for verb in what_mary_did]\n",
    "generated_event_structures = [s.sentenceToEventDict(sem, sentence) for sentence in what_mary_did_sentences]\n",
    "print generated_event_structures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree(Start[], [Tree(S[], [Tree(NP[-pro, -wh], [Tree(Name[], ['Mary'])]), Tree(VP[], [Tree(V_args[], [Tree(V2[+tense], ['ate']), Tree(NP[-pro, -wh], [Tree(Det[], ['the']), Tree(APX[], []), Tree(N[-mass, number='singular'], ['tomato'])])])])])])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.parse_input_str(sem, what_mary_did_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Start[]\n",
      "  (S[]\n",
      "    (NP[-pro, -wh] (Name[] Mary))\n",
      "    (VP[]\n",
      "      (V_args[]\n",
      "        (V2[+tense] ate)\n",
      "        (NP[-pro, -wh]\n",
      "          (Det[] the)\n",
      "          (APX[] )\n",
      "          (N[-mass, number='singular'] tomato))))))\n"
     ]
    }
   ],
   "source": [
    "tree = s.parse_input_str(sem, what_mary_did_sentences[0])\n",
    "print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ate', V2[+tense])\n",
      "\n",
      "[ *type* = 'V2' ]\n",
      "[ tense  = True ]\n"
     ]
    }
   ],
   "source": [
    "subtree = tree.pos()[1] # 2nd element of sentence was blank\n",
    "print subtree\n",
    "\n",
    "print \"\"\n",
    "features = subtree[1]\n",
    "print features\n",
    "#print tree.leaf_treeposition(1)\n",
    "#print tree[0][1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Ate', u'eat', u'eat', u'feed', u'eat', u'eat', u'eat_on', u'consume', u'eat_up', u'use_up', u'eat', u'deplete', u'exhaust', u'run_through', u'wipe_out', u'corrode', u'eat', u'rust', u'Ate', u'eat', u'eat', u'feed', u'eat', u'eat', u'eat_on', u'consume', u'eat_up', u'use_up', u'eat', u'deplete', u'exhaust', u'run_through', u'wipe_out', u'corrode', u'eat', u'rust'] \n",
      "\n",
      "[ *type* = 'V2' ]\n",
      "[ tense  = True ] \n",
      "\n",
      "[ *type* = 'V2' ]\n",
      "[ tense  = True ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'Mary _ the tomato'\n",
    "filler_index = test_sentence.split().index('_')\n",
    "print synonyms, '\\n'\n",
    "\n",
    "for sentence in what_mary_did_sentences:\n",
    "    tree = s.parse_input_str(sem, sentence)\n",
    "    features = tree.pos()[filler_index][1]\n",
    "    print features, '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the features, we can use these to inform the filtering of our synonyms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['white_potato', 'murphy', 'tater', 'white_potato', 'white_potato_vine', 'love_apple', 'tomato_plant']\n"
     ]
    }
   ],
   "source": [
    "def findPOS(features):\n",
    "    feat_keys = features.keys()\n",
    "    feat_type_index = filter(lambda (i,x): type(x) is nltk.featstruct.Feature, enumerate(feat_keys))[0][0]\n",
    "    feat_type = feat_keys[feat_type_index]\n",
    "    POS = features[feat_type]\n",
    "    return POS\n",
    "\n",
    "def getSynonymsSingleWord(word, POS):\n",
    "    if POS == 'N':\n",
    "        return flatten([synset.lemma_names() for synset in wn.synsets(word, wn.NOUN)])\n",
    "    elif POS[0] == 'V':\n",
    "        return flatten([synset.lemma_names() for synset in wn.synsets(word, wn.VERB)])\n",
    "    return flatten([synset.lemma_names() for synset in wn.synsets(word, wn.NOUN)]) #future work\n",
    "        \n",
    "    \n",
    "def getSynonyms(filler_words, POS):\n",
    "    return flatten(map(lambda word: getSynonymsSingleWord(word, POS), filler_words))\n",
    "\n",
    "def conjugate(word, POS_info):\n",
    "    features, subj_features = POS_info['features'], POS_info['subj_features']\n",
    "    POS, subj_POS = POS_info['POS'], POS_info['subj_POS']\n",
    "    conj = None\n",
    "    if POS[0] == 'V': #Verb\n",
    "        if features['tense']:\n",
    "            if subj_POS == 'N' and subj_features['number'] == 'plural':\n",
    "                conj = en.verb.conjugate(word, tense=\"present plural\")\n",
    "            elif subj_POS == 'N' or subj_POS == 'Name':\n",
    "                conj = en.verb.conjugate(word, tense=\"3rd singular present\")\n",
    "        else:\n",
    "            conj = en.verb.infinitive(word)\n",
    "    elif POS == 'N':\n",
    "        if features['number'] == 'singular':\n",
    "            conj = en.noun.singular(word)\n",
    "        elif features['number'] == 'plural':\n",
    "            conj = en.noun.plural(word)\n",
    "    elif POS == 'Name':\n",
    "        conj = en.noun.singular(word)\n",
    "    if not conj:\n",
    "        conj = word\n",
    "    return conj\n",
    "\n",
    "def fillerWordToPOSInfo(filler_word, gap_sentence):\n",
    "    filler_index = gap_sentence.split().index('_')\n",
    "    filled_sentence = gap_sentence.replace('_',filler_word)\n",
    "    tree = s.parse_input_str(sem, filled_sentence)\n",
    "    features, subj_features = tree.pos()[filler_index][1], tree.pos()[0][1]\n",
    "    POS, subj_POS = findPOS(features), findPOS(subj_features)\n",
    "    POS_info = {'features':features, 'subj_features':subj_features, \n",
    "                'POS': POS, 'subj_POS':subj_POS}\n",
    "    return POS_info\n",
    "        \n",
    "#conjugate needs all info, but generalize should only need one POS\n",
    "def generalizeAndConjugate(sem, gap_sentence, filler_words, generalizationProcedure): \n",
    "    POS_info = fillerWordToPOSInfo(filler_words[0], gap_sentence)\n",
    "    \n",
    "    generalizations = generalizationProcedure(filler_words, POS_info['POS'])\n",
    "    conjugated_gens = []\n",
    "    for g in generalizations:\n",
    "        try:\n",
    "            conjugated_gens.append(conjugate(g, POS_info))\n",
    "        except Exception as e: pass\n",
    "    return filterBySyntax(conjugated_gens)\n",
    "\n",
    "gap_sentences = ['Mary _ the tomato', 'Mary ate the _']#, '_ ate the tomato']\n",
    "for gap_sentence in gap_sentences:\n",
    "    filler_word_guesses = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "    gens = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getSynonyms)\n",
    "    print gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary ate the tomato\n",
      "Mary ate the tomato\n",
      "Mary ate the white_potato\n",
      "Mary ate the murphy\n",
      "Mary ate the tater\n",
      "Mary ate the white_potato\n",
      "Mary ate the white_potato_vine\n",
      "Mary ate the love_apple\n",
      "Mary ate the tomato_plant\n",
      "Mary ate the potato\n",
      "Mary ate the tomato\n",
      "Count:  11 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "filled_sentences = []\n",
    "for gap_sentence in gap_sentences:\n",
    "    filler_word_guesses = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "    gens = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getSynonyms)\n",
    "    for g in gens:\n",
    "        filled_sentences.append(gap_sentence.replace('_',g))\n",
    "    for fill in filler_word_guesses:\n",
    "        filled_sentences.append(gap_sentence.replace('_',fill))\n",
    "\n",
    "for x in filled_sentences:\n",
    "    print x\n",
    "print \"Count: \", len(filled_sentences), '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way of filtering down our suggested generalizations, we are more free to experiment with different generalization techniques.\n",
    "\n",
    "For example, what if we notice that the is something in common to many of the suggestions, whether they be tomato or potato?  Fundamentally, \"Mary ate the \\_\" should semantically match any food item.\n",
    "\n",
    "Following this idea, we introduce generalization by shared hypernymy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getSharedHypernyms(filler_words): #only use on Nouns\n",
    "    pairwise_synsets = []\n",
    "    for i in range(len(filler_words)):\n",
    "        for j in range(i+1,len(filler_words)):\n",
    "            a,b = filler_words[i], filler_words[j]\n",
    "            pairwise_synsets.append(findLowest(a, b)[0])\n",
    "    return reduce(lambda x,y: x.lowest_common_hypernyms(y)[0], pairwise_synsets).lemma_names()\n",
    "\n",
    "def getEntailments(filler_words): #only use on Verbs\n",
    "    synsets = flatten([wn.synsets(filler_word, wn.VERB) for filler_word in filler_words])\n",
    "    ents = flatten(map(lambda syn: syn.entailments(), synsets))\n",
    "    return flatten([ent.lemma_names() for ent in ents])\n",
    "\n",
    "def getSharedHypernymsOrEntailments(filler_words, POS):\n",
    "    if POS == 'N':\n",
    "        return getSharedHypernyms(filler_words)\n",
    "    elif POS[0] == 'V':\n",
    "        return getEntailments(filler_words)\n",
    "    return getSharedHypernyms(filler_words) #future work\n",
    "\n",
    "def findLowest(w1, w2): #only use on Nouns\n",
    "    a, b = wn.synsets(w1, wn.NOUN), wn.synsets(w2, wn.NOUN)\n",
    "        \n",
    "    low_depth, low_synset = -float('inf'), None\n",
    "    curr_x, curr_y = None, None\n",
    "    for x in a:\n",
    "            for y in b:\n",
    "                    syns = x.lowest_common_hypernyms(y)\n",
    "                    if syns:\n",
    "                        depth = syns[0].min_depth()\n",
    "                        if (depth and depth >= low_depth):\n",
    "                                low_depth = depth\n",
    "                                low_synset = syns\n",
    "                                curr_x, curr_y = x, y\n",
    "    return low_synset\n",
    "\n",
    "gap_sentence = 'Mary ate the _'\n",
    "filler_word_guesses = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getSharedHypernymsOrEntailments)\n",
    "\n",
    "gap_sentence = 'Mary _ the tomato'\n",
    "filler_word_guesses = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getSharedHypernymsOrEntailments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These generalizations are good, but do not cover very much ground. We are trying to create a long list of viable alternatively for a blank spot, and this method took us from two hypotheses to two hypotheses. We can ameliorate this by taking the transitive closure of hyponymy under these objects. Such a closure would be a generalization of each of our previous generalizations in this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aquatic_plant', 'water_plant', 'hydrophyte', 'hydrophytic_plant', 'bulbous_plant', 'cormous_plant', 'cultivar', 'cultivated_plant', 'deciduous_plant', 'desert_plant']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def getHyponymClosure(filler_words): #Noun\n",
    "    pairwise_synsets = []\n",
    "    for i in range(len(filler_words)):\n",
    "        for j in range(i+1,len(filler_words)):\n",
    "            a,b = filler_words[i], filler_words[j]\n",
    "            pairwise_synsets.append(findLowest(a, b)[0])\n",
    "    hypernym_synset = reduce(lambda x,y: x.lowest_common_hypernyms(y)[0], pairwise_synsets)\n",
    "    closure = flatten(hypernym_synset.closure(lambda y: y.hyponyms()))\n",
    "    return flatten(map(lambda x: x.lemma_names(), closure))\n",
    "\n",
    "def getHyponymsClosureOrEntailments(filler_words, POS):\n",
    "    if POS == 'N':\n",
    "        return getHyponymClosure(filler_words)\n",
    "    elif POS[0] == 'V':\n",
    "        return getEntailments(filler_words)\n",
    "    return getHyponymClosure(filler_words) #future work\n",
    "\n",
    "gap_sentence = 'Mary ate the _'\n",
    "filler_word_guesses = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "hyponym_closure = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getHyponymsClosureOrEntailments)\n",
    "print hyponym_closure[:10]\n",
    "\n",
    "gap_sentence = 'Mary _ the tomato'\n",
    "filler_word_guesses = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "hyponym_closure = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getHyponymsClosureOrEntailments)\n",
    "print hyponym_closure[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyponym closure of shared hyponyms works very well for nouns. However, when we apply the same technique on verbs, we find that many times there are no hypernyms. We may choose to be selective -- using hyponym closure for nouns and entailment for verbs.\n",
    "\n",
    "We can fill the currently unused word_type argument of getHyponymClosure to  implement differening generalization behavior based on the type of word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './Input/training.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-9d2045c7c66f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Output/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentences_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtraining_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgap_sentences_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './Input/training.txt'"
     ]
    }
   ],
   "source": [
    "training_sentences_file = 'Input/training.txt'\n",
    "gap_sentences_file = 'Input/testing.txt'\n",
    "output_dir = 'Output/'\n",
    "\n",
    "with open(training_sentences_file, 'r') as f:\n",
    "    training_sentences = [x.strip() for x in f]\n",
    "with open(gap_sentences_file, 'r') as f:\n",
    "    gap_sentences = [x.strip() for x in f]\n",
    "\n",
    "sem = semantic_rule_set.SemanticRuleSet()\n",
    "sem = syntactic_and_semantic_rules.addLexicon(sem)\n",
    "\n",
    "def fillInTheGaps(training_sentences, gap_sentences, groupingProcedure, generalizationProcedure):\n",
    "    event_groupings = train(sem, training_sentences, groupingProcedure)\n",
    "    with open(output_dir+groupingProcedure.__name__+generalizationProcedure.__name__+\".txt\", 'w+') as f:\n",
    "        for gap_sentence in gap_sentences:\n",
    "            filler_word_guesses = gapSentenceToFillerWordGuesses(sem, gap_sentence, event_groupings)\n",
    "            gens = generalizeAndConjugate(sem, gap_sentence, filler_word_guesses, getSynonyms)\n",
    "            for g in gens:\n",
    "                f.write(gap_sentence.replace('_', g)+'\\n')\n",
    "\n",
    "fillInTheGaps(training_sentences, gap_sentences, groupIfOneDiff, getSynonyms)\n",
    "fillInTheGaps(training_sentences, gap_sentences, groupIfOneOrTwoDiffs, getSynonyms)\n",
    "fillInTheGaps(training_sentences, gap_sentences, groupIfOneDiff, getHyponymsClosureOrEntailments)\n",
    "fillInTheGaps(training_sentences, gap_sentences, groupIfOneOrTwoDiffs, getHyponymsClosureOrEntailments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the types of sentence that it supports (eg adjective) -- the POS section ad-hoc (could use fxn approximator)\n",
    "\n",
    "Grouping procedures that use synsets during the iterative grouping. Grouping procedures that only group according to n/log(n) rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO -- make separate POS for each\n",
    "add entailment for verbs\n",
    "\n",
    "Assumptions\n",
    " only replaced by same part of speech\n",
    " only implemented for noun or verb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
