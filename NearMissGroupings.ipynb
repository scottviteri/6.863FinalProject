{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic as s\n",
    "import semantic_rule_set\n",
    "import syntactic_and_semantic_rules\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "training_sentences_file = 'training.txt'\n",
    "testing_sentences_file = 'testing.txt'\n",
    "show_database = False\n",
    "validate = False\n",
    "gui = False\n",
    "\n",
    "with open(training_sentences_file, 'r') as f:\n",
    "    training_sentences = [x.strip() for x in f]\n",
    "with open(testing_sentences_file, 'r') as f:\n",
    "    testing_sentences = [x.strip() for x in f]\n",
    "\n",
    "sem = semantic_rule_set.SemanticRuleSet()\n",
    "sem = syntactic_and_semantic_rules.addLexicon(sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the missing word\n",
    "\n",
    "Our goal is take a sentence with a missing word -- \n",
    " for example \"Mary ate a \\_\", and replace the blank with a 'reasonable' replacement word.\n",
    "This task is something that people do everyday, especially in a noisy setting or when speaking to someone with a thick accent. \n",
    "However, this task goes beyond just syntactic validity, since few people would guess that the sentence was 'Mary ate a laptop'. We need to inject some notion of semantics. However, we cannot just naively apply methods from WordNet, because we are missing the word that fits in the blank. Most WordNet methods are ways of mapping from one word to other words that are related in a particular way, eg hypernymy or synonymy. \n",
    "\n",
    "So in order to move forward, we should first come up with a way of distinguishing valid sentences in a way that lets us generate a missing word. We decided to adopt a model of language learning that is very similar to the notion of near-miss learning. We take a set of training sentences, use software from lab 3 to convert them into event structures, and group event structures together in a way that lets us generalize semantically valid sentences from our training data. This means that we are assuming that the training data is semantically valid. \n",
    "\n",
    "Below, we will experiment with different grouping and generalizationg strategies in order to determine semantically valid replacements for a missing word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Event Structures\n",
    "First we generate event structures from sentences, which we store in a list of dictionaries for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John ate the potato', 'John ate the tomato', 'Mary ate the tomato']\n",
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'Mary'}\n"
     ]
    }
   ],
   "source": [
    "print training_sentences\n",
    "events = map(lambda sent: s.sentenceToEventDict(sem, sent), training_sentences)\n",
    "for e in events:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest Strategy: No Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'Mary'}\n"
     ]
    }
   ],
   "source": [
    "#Parse each sentence in training data\n",
    "def train(sem, sentences, groupEvents):\n",
    "    event_list = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "           new_event_dict = s.sentenceToEventDict(sem, sentence)\n",
    "           event_list = groupEvents(event_list, new_event_dict)\n",
    "        except Exception as e:\n",
    "            # The parser did not return any parse trees.\n",
    "            raise\n",
    "    return event_list\n",
    "\n",
    "def keepSeparate(event_list, new_event_dict):\n",
    "    return event_list + [new_event_dict]\n",
    "\n",
    "event_groupings = train(sem, training_sentences, keepSeparate)\n",
    "for g in event_groupings:\n",
    "    print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Difference Groupings\n",
    "This is a very conservative form of grouping. If two training sentences the same event structure but differ across one feature, we group together the values of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'Mary'}\n",
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}\n"
     ]
    }
   ],
   "source": [
    "def groupIfOneDiff(event_list, new_event): #if different structure, do not match\n",
    "    #maybe only do after reaching a certain size\n",
    "    new_event_list = copy.deepcopy(event_list)\n",
    "    merged = False\n",
    "    #try merging in\n",
    "    for i in range(len(event_list)): #try to match with event_list[i]\n",
    "        event = event_list[i]\n",
    "        if set(event.keys()) == set(new_event.keys()):\n",
    "            unequal_count = 0\n",
    "            for feat in event.keys():\n",
    "                if new_event[feat] not in event[feat]:\n",
    "                    unequal_feat = feat\n",
    "                    unequal_count += 1\n",
    "            if unequal_count == 0: merged = True\n",
    "            elif unequal_count == 1: #merge into previous\n",
    "                new_event_list[i][unequal_feat].add(new_event[unequal_feat])\n",
    "                merged = True\n",
    "    #make new spot\n",
    "    if not merged:\n",
    "        new_event_list.append({k:set([v]) for k,v in new_event.iteritems()})\n",
    "    return new_event_list\n",
    "\n",
    "event_groupings = train(sem, training_sentences, groupIfOneDiff)\n",
    "for e in events:\n",
    "    print e\n",
    "for g in event_groupings:\n",
    "    print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above only produces one output grouping (as opposed to two groupings, composed of sentences (1,2) and (2,3)).\n",
    "This is because we are applying the groupings iteratively. We loop through the events, and compare the current event with the groupings that we have collected up to that point. The comparison in this case is not checking for equality between values of a common feature, but rather it is checking for inclusion of the current event's feature values within groupings of that feature.\n",
    "```python\n",
    "for feat in event.keys():\n",
    "    if new_event[feat] not in event[feat]:\n",
    "        unequal_feat = feat\n",
    "```\n",
    "This part of the previous method demonstrates this inclusion checking.\n",
    "\n",
    "This implies that the same training sentences, in different orders, can lead to different event groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John ate the potato', 'John ate the tomato', 'Mary ate the tomato']\n",
      "['Mary ate the tomato', 'John ate the tomato', 'John ate the potato']\n",
      "[{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}]\n",
      "[{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato']), 'agent': set(['John', 'Mary'])}, {'action': set(['eat']), 'patient': set(['tomato', 'potato']), 'tense': set(['past']), 'agent': set(['John'])}]\n"
     ]
    }
   ],
   "source": [
    "def rotate(lst): \n",
    "    return [lst[-1]] + lst[:-1]\n",
    "print training_sentences\n",
    "print training_sentences[::-1]\n",
    "event_groupings_1 = train(sem, training_sentences, groupIfOneDiff)\n",
    "event_groupings_2 = train(sem, rotate(training_sentences), groupIfOneDiff)\n",
    "\n",
    "print event_groupings_1 #creates 1 group\n",
    "print event_groupings_2 #creates 2 groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grouping pattern is related to near-miss learning:\n",
    "\n",
    "The reason that event_groupings_2 creates 2 instead of 1 grouping is that the 1st and 2nd sentence differ from each other is 2 ways, instead of just 1.\n",
    "    \n",
    "But maybe this is a bit too conservative of an assumption. Alternately we could try grouping when seeing two differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'Mary'}\n",
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'John'}\n",
      "{'action': 'eat', 'patient': 'tomato', 'tense': 'past', 'agent': 'John'}\n",
      "[{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}]\n"
     ]
    }
   ],
   "source": [
    "def groupIfOneOrTwoDiffs(event_list, new_event): #if different structure, do not match\n",
    "    #maybe only do after reaching a certain size\n",
    "    new_event_list = copy.deepcopy(event_list)\n",
    "    merged = False\n",
    "    #try merging in\n",
    "    for i in range(len(event_list)): #try to match with event_list[i]\n",
    "        event = event_list[i]\n",
    "        if set(event.keys()) == set(new_event.keys()):\n",
    "            unequal_count = 0\n",
    "            for feat in event.keys():\n",
    "                if new_event[feat] not in event[feat]:\n",
    "                    if unequal_count == 0:\n",
    "                        unequal_feat_1 = feat\n",
    "                    if unequal_count == 1:\n",
    "                        unequal_feat_2 = feat\n",
    "                    unequal_count += 1\n",
    "            if unequal_count == 0: merged = True\n",
    "            elif unequal_count == 1: #merge into previous\n",
    "                new_event_list[i][unequal_feat_1].add(new_event[unequal_feat_1])\n",
    "                merged = True\n",
    "            elif unequal_count == 2: #merge into previous\n",
    "                new_event_list[i][unequal_feat_1].add(new_event[unequal_feat_1])\n",
    "                new_event_list[i][unequal_feat_2].add(new_event[unequal_feat_2])\n",
    "                merged = True\n",
    "    #make new spot\n",
    "    if not merged:\n",
    "        new_event_list.append({k:set([v]) for k,v in new_event.iteritems()})\n",
    "    return new_event_list\n",
    "\n",
    "events = map(lambda sent: s.sentenceToEventDict(sem, sent), rotate(training_sentences))\n",
    "for e in events:\n",
    "    print e\n",
    "event_groupings = train(sem, rotate(training_sentences), groupIfOneOrTwoDiffs)\n",
    "print event_groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling In the Word Blank\n",
    "Now that we have come up with some event grouping strategies, we return to the original goal of our project -- to fill in the missing word.\n",
    "\n",
    "The reason for the groupings above, is that we would like to take training event structures like:\n",
    "```python\n",
    "['John ate the potato', 'John ate the tomato', 'Mary ate the tomato']\n",
    "```\n",
    "And conclude that:\n",
    "```python\n",
    "['Mary ate the potato']\n",
    "```\n",
    "is a valid sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'eat', 'patient': 'potato', 'tense': 'past', 'agent': 'Mary'}\n",
      "[{'action': set(['eat']), 'tense': set(['past']), 'patient': set(['tomato', 'potato']), 'agent': set(['John', 'Mary'])}]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def checkGoodSentence(sem, sentence, event_list):\n",
    "    event = s.sentenceToEventDict(sem, sentence)\n",
    "    if not event: return False\n",
    "    for event_group in event_list:\n",
    "        if set(event.keys()) == set(event_group.keys()):\n",
    "            if all([event[k] in event_group[k] for k in event.keys()]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "sentence = 'Mary ate the potato'\n",
    "event_groupings = train(sem, training_sentences, groupIfOneDiff)\n",
    "event = s.sentenceToEventDict(sem, sentence)\n",
    "print event\n",
    "print event_groupings\n",
    "\n",
    "print checkGoodSentence(sem, sentence, event_groupings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the above means is that the grouping structure that we have generated 'accepts' the sentence 'Mary ate the potato' after being trained on the 3 sentences above, which is exactly what we were looking for!\n",
    "\n",
    "However, we want to be able to hypothesize that 'potato' is a good word to fill in for 'Mary ate the \\_'. So instead of starting with 'Mary ate the potato', let's start with 'Mary ate the \\_', and check the semantic validity of every word in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mary ate the _': ['Mary ate the potato', 'Mary ate the tomato']}\n"
     ]
    }
   ],
   "source": [
    "def test(sem, sentences, event_list):\n",
    "    results = {}\n",
    "    guess_words = s.getTerminals(sem) #all words in the lexicon\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        filler_i = words.index('_')\n",
    "        without_filler = words[:filler_i] + words[filler_i+1:] #Mary ate the _\n",
    "        good_hypotheses = []\n",
    "        for guess_word in guess_words:\n",
    "            all_words = without_filler[:filler_i] + [guess_word] + without_filler[filler_i:]\n",
    "            guess_sentence = ' '.join(all_words)\n",
    "            if checkGoodSentence(sem, guess_sentence, event_list):\n",
    "                good_hypotheses.append(guess_sentence)\n",
    "        results[sentence] = good_hypotheses\n",
    "    return results\n",
    "\n",
    "results = test(sem, ['Mary ate the _'], event_groupings)\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is doing what we want, but ideally we would like to extend our groupings to more than just the lexicon that we have written down.\n",
    "\n",
    "This is where we can use wordnet to generalize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary ate the potato\n",
      "Mary ate the white_potato\n",
      "Mary ate the Irish_potato\n",
      "Mary ate the murphy\n",
      "Mary ate the spud\n",
      "Mary ate the tater\n",
      "Mary ate the potato\n",
      "Mary ate the white_potato\n",
      "Mary ate the white_potato_vine\n",
      "Mary ate the Solanum_tuberosum\n",
      "Mary ate the tomato\n",
      "Mary ate the tomato\n",
      "Mary ate the love_apple\n",
      "Mary ate the tomato_plant\n",
      "Mary ate the Lycopersicon_esculentum\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def flatten(lst):\n",
    "    out = []\n",
    "    for x in lst:\n",
    "        if type(x) is list:\n",
    "            out.extend(flatten(x))\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "filler_words = map(lambda x:x.split()[-1], results['Mary ate the _'])\n",
    "synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w)), filler_words))\n",
    "\n",
    "sents = ['Mary ate the '+syn for syn in synonyms]\n",
    "for x in sents: print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary Ate the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat the tomato\n",
      "Mary feed the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat_on the tomato\n",
      "Mary consume the tomato\n",
      "Mary eat_up the tomato\n",
      "Mary use_up the tomato\n",
      "Mary eat the tomato\n",
      "Mary deplete the tomato\n",
      "Mary exhaust the tomato\n",
      "Mary run_through the tomato\n",
      "Mary wipe_out the tomato\n",
      "Mary corrode the tomato\n",
      "Mary eat the tomato\n",
      "Mary rust the tomato\n",
      "Mary Ate the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat the tomato\n",
      "Mary feed the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat the tomato\n",
      "Mary eat_on the tomato\n",
      "Mary consume the tomato\n",
      "Mary eat_up the tomato\n",
      "Mary use_up the tomato\n",
      "Mary eat the tomato\n",
      "Mary deplete the tomato\n",
      "Mary exhaust the tomato\n",
      "Mary run_through the tomato\n",
      "Mary wipe_out the tomato\n",
      "Mary corrode the tomato\n",
      "Mary eat the tomato\n",
      "Mary rust the tomato\n"
     ]
    }
   ],
   "source": [
    "results = test(sem, ['Mary _ the tomato'], event_groupings)\n",
    "filler_words = map(lambda x:x.split()[1], results['Mary _ the tomato'])\n",
    "synonyms = flatten(map(lambda w: map(lambda x: x.lemma_names(), wn.synsets(w)), filler_words))\n",
    "\n",
    "sents = ['Mary '+syn+' the tomato' for syn in synonyms]\n",
    "for x in sents: print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we took each candidate word, and took the union of all synsets of those words. However, this may over-generate, since some synsets of a particular noun might be verbs. We see above that the problem is especially acute for verbs, where we lose subject-verb agreement.\n",
    "\n",
    "solns -- hit with nltk parser afterwards, or be picky with wordnet category (info in lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 -- change different words\n",
    "#2 -- look at different wordnet schemes\n",
    "\n",
    "# potentially filter by part of sentence when expanding synsets of lemmas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
